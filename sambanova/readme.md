# Sambanova Getting Started

## On Boarding

See [ALCF AI Testbed User Guide](https://www.alcf.anl.gov/support-center/get-started) to request an account and additional information.

## System View

Connection to a SambaNova node is a two-step process. The first step is to `ssh` to the `login node`. This step requires an MFA passcode for authentication. Eight-digit passcode generated by an app on your mobile device, e.g., mobilePASS+.
The second step is to log in to a SambaNova node from the `login node`.

![SambaNova System View](sambanova_login_new.jpg "SambaNova System View")

## Connect to Login Node

* Login to the SambaNova login node from your local machine using the below command. This uses the MobilePass+ token generated every time you log in to the system. 

    In the examples below, replace `ALCFUserID` with your ALCF user id.

    ```bash
    $ ssh ALCFUserID@sambanova.alcf.anl.gov
    ALCFUserID@sambanova.alcf.anl.govs password: < MobilePass+ code >
    ```

    Note: Use the ssh "-v" option in order to debug any ssh problems.


* Once you are on the login node, the SambaNova system can be accessed using the alias `sm-01` or `sm-02`.

    ```bash
    $ ssh sm-01
    # or
    $ ssh sm-02
    ```

    **NOTE:** There is just one scheduler for both sm-01 and sm-02. Run the python scripts using srun or sbatch, to ensure that concurrent jobs do not interfere with each other.

## SambaFlow SDK setup

* The SambaNova system has a bash shell script to set up the required software environment. This sets up the SambaFlow software stack, and the associated environmental variables and starts a pre-configured virtual environment.

    ```bash
    ALCFUserID@sm-01:~$ source /software/sambanova/envs/sn_env.sh
    ```

* The contents of the `sn_env.sh` script is shown below for convenience.

  * Setup the path to SambaFlow software stack. This stack includes the Runtime, the compilers, and the SambaFlow Python SDK which is used to create and run models.
      ```bash
      alias snpath='export PATH=$PATH:/opt/sambaflow/bin' 
      ```

  * The `OMP_NUM_THREADS` environment variable sets the number of threads to use for parallel regions. The value of this environment variable must be a list of positive integer values. The values of the list set the number of threads to use for parallel regions at the corresponding nested levels. For the SambaNova system, it is usually set to 1.
      ```bash
      alias snthreads='export OMP_NUM_THREADS=16'
      ```

  * This starts the pre-configured virtual environment that consists of sambaflow and other built-in libraries.
      ```bash
      alias snvenv='source /opt/sambaflow/venv/bin/activate' 
      ```

    **Note:**Â  SambaNova operations will fail unless the SambaNova `venv` is set up.

* You may deactivate the environment when finished.
    ```bash
    $ deactivate
    ```

## BERT-Large on Sambanova

* BERT Code is in the [Bert](./bert/) directory here.  
  * [transformners_hook.py](./bert/transformers_hook.py): contains code for BERT.
  * [bert_train-inf.sh](./bert//bert_train-inf.sh): contains driver code to run and profile BERT. 
  * For your convenience, these scripts are also available locally on each machine. 
    Copy them to your `$HOME` directory. 
    ```bash
    $ cp /var/tmp/Additional/slurm/Models/ANL_Acceptance_RC1_11_5/bert_train-inf.sh ~/
    ```

* SambaNova uses Slurm for job submission and queueing. We will use `srun` to submit our job to job scheduler. Please refer to [Sambanova Documentation](https://www.alcf.anl.gov/support/ai-testbed-userdocs/sambanova/Job-Queuing-and-Submission/index.html) for further details. 
    ```bash
    $ cd ~
    $ srun bert_train-inf.sh
    ```


# Steps to run BERT-Large on Sambanova

## Getting started

1. Login to Sambanova login node. 
    ```bash
      $ ssh ALCFUserID@sambanova.alcf.anl.gov
    ```

2. BERT Code is in the [Bert](./bert/) directory of this repository. For your convenience, this code is also available locally. 
   Copy BERT code to youyr `$HOME` directory. 
    ```bash
    $ cp -r /projects/aitestbed_training/SN/bert ~/  
    $ cd bert  
    ```
    * [transformners_hook.py](./bert/transformers_hook.py): contains code for BERT.
    * [bert_train-inf.sh](./bert//bert_train-inf.sh): contains driver code to run and profile BERT. 

3. Connect to the one of the destination Sambanova nodes:  
    ```bash
    $ ssh sm-01 
    or 
    $ ssh sm-02
    ```

4. SambaNova uses SLURM for job submission and queueing. We will use `srun` to submit our job to job scheduler. Please refer to [Sambanova Documentation](https://www.alcf.anl.gov/support/ai-testbed-userdocs/sambanova/Job-Queuing-and-Submission/index.html) for further details. 
  
    ```bash
    $ cd ~
    $ srun bert_train-inf.sh
    ```

    <details>
    <summary>Click for Sample Output</summary>

    ```bash
    $ ToDo
    $ ToDo
    $ ToDo
    $ ToDo
    $ ToDo
    ```

    </details>


* **Run precompiled model:**
To further reduce the execution time we have precompiled model. For your conveneice, the precompiled model is available at `/projects/aitestbed_training/SN/precompiled_bert`. Here are commands to run it. 

    ```bash
    $ TODO
    ```
    This will take approximately `#todo` minutes to run. Here is the sample outout 

    <details>
    <summary>Click for Sample Output</summary>

    ```bash
    $ ToDo
    $ ToDo
    $ ToDo
    $ ToDo
    $ ToDo
    ```

    </details>

## Pretraining in data parallel mode

1. Create a folder for pretraining in your home repo, and copy the bash script `/projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-pretrain-job-LBS1024.sh` to it. Then, go to that folder. Example:

   ```bash
      $ cd $HOME
      $ mkdir pretrain
      $ cp /projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-pretrain-job-LBS1024.sh pretrain/
      $ cd pretrain/
      ```

2. Open the `submit-bert-pretrain-job-LBS1024.sh` file, and change `OUTDIR` to location of the pretrain folder. Example: `OUTDIR=$HOME/pretrain`

3. Submit your job (2 RDUs are used here). Example: 

   ```bash
      $ sbatch --output=log_bert_pretrain_LBS1024_np2.out --gres=rdu:2 -c 8 submit-bert-pretrain-job-LBS1024.sh
      ```
   Note: `-c` represents the number of cores per task  
      
4. You can follow the status of your job using: `squeue`. The job should take about 5 min to complete.

5. Once the job is completed, you can see the checkpoint(s) and accuracy metrics in `hf_output_lrg_run/`. The throughput is outputted in the `log_bert_pretrain_LBS1024_np2.out` file (search for throughput in the file).

## Fine-tuning for question answering using 1 RDU:

1. Create a folder for finetuning in your home repo, and copy the bash script `/projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-squad-job.sh` to it. Then, go to that folder. Example:

   ```bash
      $ cd $HOME
      $ mkdir finetune
      $ cp /projects/aitestbed_training/SN/precompiled_bert/bash_scripts/submit-bert-squad-job.sh finetune/
      $ cd finetune/
      ```

2. Open the `submit-bert-pretrain-job-LBS1024.sh` file, and change `OUTDIR` to location of the finetune folder. Example: `OUTDIR=$HOME/finetune`

3. Submit your job (1 RDU is used here). Example: 

   ```bash
      $ sbatch --output=log_bert_squad.out --gres=rdu:1 -c 8 submit-bert-squad-job.sh
      ```
      
4. You can follow the status of your job using: `squeue`. The job should take about 20 min to complete. Note: for finetuning, data is not pre-tokenized and will be tokenized on the fly, that is why the job is taking longer to complete.

5. Once the job is completed, you can see the checkpoint(s) and accuracy metrics in `hf_output_squad_run/`.
