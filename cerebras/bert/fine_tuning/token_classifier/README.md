# TensorFlow BERT fine-tuning token classification model

- [TensorFlow BERT fine-tuning token classification model](#tensorflow-bert-fine-tuning-token-classification-model)
  - [Model overview](#model-overview)
  - [Sequence of steps to perform](#sequence-of-steps-to-perform)
  - [Key features from CSoft platform used in this reference implementation](#key-features-from-csoft-platform-used-in-this-reference-implementation)
  - [Code structure](#code-structure)
  - [Dataset generation](#dataset-generation)
    - [Data download](#data-download)
    - [Data preparation](#data-preparation)
    - [Input function description](#input-function-description)
      - [Features dictionary and labels](#features-dictionary-and-labels)
  - [Compile model](#compile-model)
    - [Validate only](#validate-only)
    - [Compile only](#compile-only)
  - [Run fine-tuning](#run-fine-tuning)
    - [Run fine-tuning on Cerebras System](#run-fine-tuning-on-cerebras-system)
    - [Run fine-tuning on GPU](#run-fine-tuning-on-gpu)
  - [Configs included for this model](#Configs-included-for-this-model)
  - [References](#references)

## Model overview

This directory contains implementation of the BERT Fine-tuning model for token classification tasks, also referred to as Named Entity Recognition (NER). We provide the BC5-chem dataset as an example from the [BLURB dataset](https://microsoft.github.io/BLURB/tasks.html). The named entity recognition task is used to tag named entities in the input text, this is labeled as `B` conventionally. Some times, the entity is broken into two or more words, then the following entities are labelled as `I`. The words which are not named entities are labelled as `O`. If an entity is broken into multiple tokens, then the subsequent tokens are labelled as `X`.

This is a fine-tuning task, meaning, the BERT model [[1](#references)] is initialized with the weights generated by pre-training the model on a masked language modeling task. The model architecture is adjusted by replacing the masked language modeling (MLM) and next sentence prediction (NSP) output heads used in pre-training with a token classification head. The new output head layer is initialized with random weights.

Note: The PyTorch version of this model is located at [PyTorch Token Classifier](../../../../pytorch/bert/fine_tuning/token_classifier/).

## Sequence of steps to perform

The following block diagram shows a high-level view of the sequence of steps you will perform in this example.

<p align = "center">
<img src = ./images/steps-tf-token-classifier.png>
</p>
<p align = "center">
Fig.1 - Flow Chart of steps to fine-tune token classifier model
</p>

## Key features from CSoft platform used in this reference implementation

Token classification fine-tuning model configs are supported in the [Layer Pipelined mode](https://docs.cerebras.net/en/latest/cerebras-basics/cerebras-execution-modes.html#layer-pipelined-mode
).

## Code structure

- `configs/`: YAML configuration files.
- `input/`: Input pipeline implementation for the above mentioned datasets.
- `BertTokenClassifierModel.py`: Model implementation. A bulk of the model is defined in this file. It inherits from the central BaseModel located in `common/BaseModel.py`. The model also uses Cerebras-defined layers that are located in `common/layers/tf`.
- `model.py`: The entry point to the model. Defines `model_fn`.
- `data.py`: The entry point to the data input pipeline code.
- `run.py`: Training script. Performs training and validation.
- `utils.py`: Miscellaneous helper functions.

## Dataset generation

### Data download

Download and pre-process the dataset from [BLURB dashboard](https://microsoft.github.io/BLURB/submit.html):

```bash
wget https://microsoft.github.io/BLURB/sample_code/data_generation.tar.gz

tar -xf data_generation.tar.gz
```

Refer to `README.md` from the `data_generation` (downloaded and extracted in [Data Download](#data-download) step) folder to download and pre-process the data for NER.

### Data preparation

Generate TF Records from the raw data using the script `write_tfrecords_ner.py`

```bash

python write_tfrecords_ner.py \
    --data_dir=/path/to/BC5CDR-chem \
    --vocab_file=/path/to/uncased_pubmed_abstracts_and_fulltext_vocab.txt \
    --output_dir=/path/to/bc5dr-chem-tfrecords \
    --do_lower_case
```

Run `python write_tfrecords_ner.py --help` for more information about the arguments.

### Input function description

The input to the model takes in one sentence for which the named entities need to be tagged. Every sentence is preceded by the `[CLS]` special token and is followed by the `[SEP]` special token. The BERT model pre-training uses a special segment embedding layer at the input to signify the two sentences used in the NSP task.

The labels for this model is a sequence of the same length as the input ids and has one of the labels as indices from `"[PAD]", "B", "I", "O", "X", "[CLS]", "[SEP]"`.

If you want to use your own input function with this example code, then this section describes the input data format expected by `BertTokenClassificationModel` class defined in [BertTokenClassificationModel.py](./BertTokenClassificationModel.py). When you create your own custom BERT Token Classifier input function, you must ensure that your input function produces a features dictionary and a label tensor as described in this section.

#### Features dictionary and labels

The model takes in a tuple of features dictionary and label array. The input features dictionary has the following key/values:

- `input_ids`: Input token IDs, padded with `0` to `max_sequence_length`. The tokens in the dataset are mapped to these IDs using the vocabulary file. These values should be between `0` and `vocab_size - 1`, inclusive. The first token should be the special `[CLS]` token. The end of each sentence should be marked by the special `[SEP]` token. So, a sentence pair should be separated by additional `[SEP]` token.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `tf.int32`

- `attention_mask`: Mask for padded positions. Has values `0` on the padded positions and `1` elsewhere.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `tf.int32`

- `token_type_ids`: Segment IDs. A tensor of the same size as the `input_ids` designating to which segment each token belongs. It is all zeros as there is only one sentence in the input sequence.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `tf.int32`

The labels have following values.

- `labels`: The labels for this model is a sequence of the same length as the `input_ids` and has one of the labels from indices corresponding to `"[PAD]", "B", "I", "O", "X", "[CLS]", "[SEP]"`.
  - Shape: `[batch_size, max_sequence_length]`
  - Type: `tf.int32`

## Compile model

We recommend that you first compile your model successfully on a support cluster CPU node before running it on the CS system.

### Validate only

You can run in `validate_only` mode that runs a fast, light-weight verification. In this mode, the compilation will only run through the first few stages, up until kernel library matching. You can run it by using the below command:

```bash
csrun_cpu python run.py \
  --mode=train \
  --params <path/to/yaml> \
  --model_dir </path/to/model_dir> \
  --validate_only 
```

After a successful `validate_only` run, you can run full compilation with `compile_only` mode.

### Compile only

This step runs the full compilation through all stages of the Cerebras software stack to generate a CS system executable. You can run it by using the below command:

```bash
csrun_cpu python run.py \
  --mode=train \
  --params <path/to/yaml> \
  --model_dir </path/to/model_dir> \
  --compile_only 
```

When the above compilation is successful, the model is guaranteed to run on CS system. You can also use this mode to run pre-compilations of many different model configurations offline, so that you can fully utilize allotted CS system cluster time.

## Run fine-tuning

**IMPORTANT**: See the following notes before proceeding further.

**Parameter settings in YAML config file**: The config YAML files are located in the [configs](configs/) directory. Before starting a pre-training run, make sure that in the YAML config file you are using:

- The `train_input.data_dir` parameter points to the correct dataset, and
- The `train_input.max_sequence_length` parameter corresponds to the sequence length of the dataset.
- The `train_input.batch_size` parameter will set the batch size for the training.
- The `train_input.label_vocab_file` generated during the [Data Preparation](#data-preparation) step.

Same applies for the `eval_input`.

**YAML config file differences**:

Please check [YAML config section](#yaml-config-file-description) for details on each config supported out of the box for this model.

### Run fine-tuning on Cerebras System

Follow [How to train on the CS System](../../../#how-to-train-on-the-cs-system) and execute the following command from within the Cerebras environment:

```bash
csrun_wse python run.py \
--mode train \
--cs_ip x.x.x.x \
--params /path/to/yaml \
--model_dir /path/to/model_dir \
--checkpoint_path /path/to/pretrained_checkpoint.mdl
```

where:

- `/path/to/yaml` is a path to the YAML config file with model parameters. A few example YAML config files can be found in [configs](configs) directory.
- `/path/to/model_dir` is a path to the directory where you would like to store the logs and other artifacts of the run.
- `cs_ip` is the IP address of the CM endpoint.
- `checkpoint_path` is the path to the saved checkpoint from BERT pre-training.

### Run fine-tuning on GPU

To run pre-training on GPU, use the `run.py` Python utility as follows:

```bash
python run.py \
--mode train \
--params /path/to/yaml \
--model_dir /path/to/model_dir \
--checkpoint_path /path/to/pretrained_checkpoint.mdl
```

The description of the command-line arguments is the same as in the above section.

## Configs included for this model

- `params_bert_base_*.yaml` have the standard bert-base config with `hidden_size=768, num_hidden_layers=12, num_heads=12` as a backbone.
- `params_bert_large_*.yaml` have the standard bert-large config with `hidden_size=1024, num_hidden_layers=24, num_heads=16` as a backbone.

## References

[1] [BERT paper](https://arxiv.org/abs/1810.04805)
