#BERT-Base for 3 epochs on SST-2

### Input
train_input:
    data_processor: Sst2DataProcessor
    data_dir: './tfds'
    vocab_file: '../../../../vocab/google_research_uncased_L-12_H-768_A-12.txt'
    max_sequence_length: 72
    add_special_tokens: True
    batch_size: 12
    shuffle: True

eval_input:
    data_processor: Sst2DataProcessor
    data_dir: './tfds'
    vocab_file: '../../../../vocab/google_research_uncased_L-12_H-768_A-12.txt'
    max_sequence_length: 72
    add_special_tokens: True
    batch_size: 12

predict_input:
    # remaining params inherited from eval_input
    batch_size: 1

model:
    pretrain_params_path: "../../configs/params_bert_base_msl512.yaml"
    cls_dropout_rate: 0.1
    num_cls_classes: 2
    mixed_precision: True
    boundary_casting: False
    tf_summary: False

### Optimization
optimizer:
    optimizer_type: "adamw" # {"sgd", "momentum", "adam", "adamw"}
    weight_decay_rate: 0.01
    epsilon: 1e-6
    max_gradient_norm: 1.0
    learning_rate:
        - scheduler: "Linear"
          steps: 2000
          initial_learning_rate: 0.0
          end_learning_rate: 0.00005
        - scheduler: "Constant"
          learning_rate: 0.00005
    loss_scaling_factor: "dynamic"
    log_summaries: True

### Runtime parameters
runconfig:
    max_steps: 20000
    save_summary_steps: 100
    save_checkpoints_steps: 2000
    eval_steps: 0 # eval_steps <= 0 means we use all of the eval data
    keep_checkpoint_max: 2
    tf_random_seed: 1202
    throttle_secs: 0
